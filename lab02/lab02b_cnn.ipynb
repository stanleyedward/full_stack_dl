{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlH0lCOttCs5"
      },
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUPRHaeetRnT"
      },
      "source": [
        "# Lab 02b: Training a CNN on Synthetic Handwriting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry3Hr-PcgDs"
      },
      "source": [
        "### What You Will Learn\n",
        "\n",
        "- Fundamental principles for building neural networks with convolutional components\n",
        "- How to use Lightning's training framework via a CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs0LXXlCU6Ix"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkQiK7lkgeXm"
      },
      "source": [
        "If you're running this notebook on Google Colab,\n",
        "the cell below will run full environment setup.\n",
        "\n",
        "It should take about three minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVx7C7H0PIZC"
      },
      "outputs": [],
      "source": [
        "lab_idx = 2\n",
        "\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the lab directory\n",
        "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "    # allow \"hot-reloading\" of modules\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    # needed for inline plots in some contexts\n",
        "    %matplotlib inline\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "\n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZN4bGgsgWc_"
      },
      "source": [
        "# Why convolutions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9HoYWZKtTE_"
      },
      "source": [
        "The most basic neural networks,\n",
        "multi-layer perceptrons,\n",
        "are built by alternating\n",
        "parameterized linear transformations\n",
        "with non-linear transformations.\n",
        "\n",
        "This combination is capable of expressing\n",
        "[functions of arbitrary complexity](http://neuralnetworksanddeeplearning.com/chap4.html),\n",
        "so long as those functions\n",
        "take in fixed-size arrays and return fixed-size arrays.\n",
        "\n",
        "```python\n",
        "def any_function_you_can_imagine(x: torch.Tensor[\"A\"]) -> torch.Tensor[\"B\"]:\n",
        "    return some_mlp_that_might_be_impractically_huge(x)\n",
        "```\n",
        "\n",
        "But not all functions have that type signature.\n",
        "\n",
        "For example, we might want to identify the content of images\n",
        "that have different sizes.\n",
        "Without gross hacks,\n",
        "an MLP won't be able to solve this problem,\n",
        "even though it seems simple enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6LjfV3o6tTFA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\" width=\"13\" height=\"13\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "import IPython.display as display\n",
        "\n",
        "randsize = 10 ** (random.random() * 2 + 1)\n",
        "\n",
        "Url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\"\n",
        "\n",
        "# run multiple times to display the same image at different sizes\n",
        "#  the content of the image remains unambiguous\n",
        "display.Image(url=Url, width=randsize, height=randsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9j6YQRftTFB"
      },
      "source": [
        "Even worse, MLPs are too general to be efficient.\n",
        "\n",
        "Each layer applies an unstructured matrix to its inputs.\n",
        "But most of the data we might want to apply them to is highly structured,\n",
        "and taking advantage of that structure can make our models more efficient.\n",
        "\n",
        "It may seem appealing to use an unstructured model:\n",
        "it can in principle learn any function.\n",
        "But\n",
        "[most functions are monstrous outrages against common sense](https://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions).\n",
        "It is useful to encode some of our assumptions\n",
        "about the kinds of functions we might want to learn\n",
        "from our data into our model's architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvC_yZvmuwgJ"
      },
      "source": [
        "## Convolutions are the local, translation-equivariant linear transforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhnRx_BZtTFC"
      },
      "source": [
        "One of the most common types of structure in data is \"locality\" --\n",
        "the most relevant information for understanding or predicting a pixel\n",
        "is a small number of pixels around it.\n",
        "\n",
        "Locality is a fundamental feature of the physical world,\n",
        "so it shows up in data drawn from physical observations,\n",
        "like photographs and audio recordings.\n",
        "\n",
        "Locality means most meaningful linear transformations of our input\n",
        "only have large weights in a small number of entries that are close to one another,\n",
        "rather than having equally large weights in all entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SSnkzV2_tTFC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generic:\n",
            "tensor([[ 1.4246],\n",
            "        [ 0.8475],\n",
            "        [-0.5180],\n",
            "        [ 0.6653],\n",
            "        [-2.3667],\n",
            "        [-1.0564],\n",
            "        [ 0.2581],\n",
            "        [-3.0556]])\n",
            "local:\n",
            "tensor([[0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.3180],\n",
            "        [0.9324],\n",
            "        [0.3779],\n",
            "        [0.0000],\n",
            "        [0.0000]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "generic_linear_transform = torch.randn(8, 1)\n",
        "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
        "\n",
        "local_linear_transform = torch.tensor([\n",
        "    [0, 0, 0] + [random.random(), random.random(), random.random()] + [0, 0]]).T\n",
        "print(\"local:\", local_linear_transform, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nCD75NwtTFD"
      },
      "source": [
        "Another type of structure commonly observed is \"translation equivariance\" --\n",
        "the top-left pixel position is not, in itself, meaningfully different\n",
        "from the bottom-right position\n",
        "or a position in the middle of the image.\n",
        "Relative relationships matter more than absolute relationships.\n",
        "\n",
        "Translation equivariance arises in images because there is generally no privileged\n",
        "vantage point for taking the image.\n",
        "We could just as easily have taken the image while standing a few feet to the left or right,\n",
        "and all of its contents would shift along with our change in perspective.\n",
        "\n",
        "Translation equivariance means that a linear transformation that is meaningful at one position\n",
        "in our input is likely to be meaningful at all other points.\n",
        "We can learn something about a linear transformation from a datapoint where it is useful\n",
        "in the bottom-left and then apply it to another datapoint where it's useful in the top-right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "srvI7JFAtTFE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generic:\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6],\n",
            "        [7]])\n",
            "translation invariant:\n",
            "tensor([[0, 7, 6, 5, 4, 3, 2, 1],\n",
            "        [1, 0, 7, 6, 5, 4, 3, 2],\n",
            "        [2, 1, 0, 7, 6, 5, 4, 3],\n",
            "        [3, 2, 1, 0, 7, 6, 5, 4],\n",
            "        [4, 3, 2, 1, 0, 7, 6, 5],\n",
            "        [5, 4, 3, 2, 1, 0, 7, 6],\n",
            "        [6, 5, 4, 3, 2, 1, 0, 7],\n",
            "        [7, 6, 5, 4, 3, 2, 1, 0]])\n"
          ]
        }
      ],
      "source": [
        "generic_linear_transform = torch.arange(8)[:, None]\n",
        "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
        "\n",
        "equivariant_linear_transform = torch.stack([torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1)\n",
        "print(\"translation invariant:\", equivariant_linear_transform, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF576NCvtTFE"
      },
      "source": [
        "A linear transformation that is translation equivariant\n",
        "[is called a _convolution_](https://en.wikipedia.org/wiki/Convolution#Translational_equivariance).\n",
        "\n",
        "If the weights of that linear transformation are mostly zero\n",
        "except for a few that are close to one another,\n",
        "that convolution is said to have a _kernel_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9tp4tBgWtTFF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[-0.2761, -0.1934, -0.1356]]], requires_grad=True)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the equivalent of torch.nn.Linear, but for a 1-dimensional convolution\n",
        "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
        "\n",
        "conv_layer.weight  # aka kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deXA_xS6tTFF"
      },
      "source": [
        "Instead of using normal matrix multiplication to apply the kernel to the input,\n",
        "we repeatedly apply that kernel over and over again,\n",
        "\"sliding\" it over the input to produce an output.\n",
        "\n",
        "Every convolution kernel has an equivalent matrix form,\n",
        "which can be matrix multiplied with the input to create the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mFoSsa5DtTFF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv_kernel_as_vector: tensor([-0.2761, -0.1934, -0.1356,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "       grad_fn=<CatBackward0>)\n",
            "convolution matrix:\n",
            "tensor([[-0.2761, -0.1934, -0.1356,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000, -0.2761, -0.1934, -0.1356,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000, -0.2761, -0.1934, -0.1356,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000, -0.2761, -0.1934, -0.1356,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2761, -0.1934, -0.1356,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2761, -0.1934, -0.1356],\n",
            "        [-0.1356,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2761, -0.1934],\n",
            "        [-0.1934, -0.1356,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2761]],\n",
            "       grad_fn=<StackBackward0>)\n"
          ]
        }
      ],
      "source": [
        "conv_kernel_as_vector = torch.hstack([conv_layer.weight[0][0], torch.zeros(5)])\n",
        "conv_layer_as_matrix = torch.stack([torch.roll(conv_kernel_as_vector, ii) for ii in range(8)], dim=0)\n",
        "print(f\"conv_kernel_as_vector: {conv_kernel_as_vector}\", sep='\\n')\n",
        "print(\"convolution matrix:\", conv_layer_as_matrix, sep=\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJyRtf9NtTFG"
      },
      "source": [
        "> <small> Under the hood, the actual operation that implements the application of a convolutional kernel\n",
        "need not look like either of these\n",
        "(common approaches include\n",
        "[Winograd-type algorithms](https://arxiv.org/abs/1509.09308)\n",
        "and [Fast Fourier Transform-based algorithms](https://arxiv.org/abs/1312.5851)).</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytivdcItTFG"
      },
      "source": [
        "Though they may seem somewhat arbitrary and technical,\n",
        "convolutions are actually a deep and fundamental piece of mathematics and computer science.\n",
        "Fundamental as in\n",
        "[closely related to the multiplication algorithm we learn as children](https://charlesfrye.github.io/math/2019/02/20/multiplication-convoluted-part-one.html)\n",
        "and deep as in\n",
        "[closely related to the Fourier transform](https://math.stackexchange.com/questions/918345/fourier-transform-as-diagonalization-of-convolution).\n",
        "Generalized convolutions can show up\n",
        "wherever there is some kind of \"sum\" over some kind of \"paths\",\n",
        "as is common in dynamic programming.\n",
        "\n",
        "In the context of this course,\n",
        "we don't have time to dive much deeper on convolutions or convolutional neural networks.\n",
        "\n",
        "See Chris Olah's blog series\n",
        "([1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/),\n",
        "[2](https://colah.github.io/posts/2014-07-Understanding-Convolutions/),\n",
        "[3](https://colah.github.io/posts/2014-12-Groups-Convolution/))\n",
        "for a friendly introduction to the mathematical view of convolution.\n",
        "\n",
        "For more on convolutional neural network architectures, see\n",
        "[the lecture notes from Stanford's 2020 \"Deep Learning for Computer Vision\" course](https://cs231n.github.io/convolutional-networks/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCJTwCWYzRee"
      },
      "source": [
        "## We apply two-dimensional convolutions to images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8RKOPAIx0O2"
      },
      "source": [
        "In building our text recognizer,\n",
        "we're working with images.\n",
        "Images have two dimensions of translation equivariance:\n",
        "left/right and up/down.\n",
        "So we use two-dimensional convolutions,\n",
        "instantiated in `torch.nn` as `nn.Conv2d` layers.\n",
        "Note that convolutional neural networks for images\n",
        "are so popular that when the term \"convolution\"\n",
        "is used without qualifier in a neural network context,\n",
        "it can be taken to mean two-dimensional convolutions.\n",
        "\n",
        "Where `Linear` layers took in batches of vectors of a fixed size\n",
        "and returned batches of vectors of a fixed size,\n",
        "`Conv2d` layers take in batches of two-dimensional _stacked feature maps_\n",
        "and return batches of two-dimensional stacked feature maps.\n",
        "\n",
        "A pseudocode type signature based on\n",
        "[`torchtyping`](https://github.com/patrick-kidger/torchtyping)\n",
        "might look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJvMdHL7w_lu"
      },
      "source": [
        "```python\n",
        "StackedFeatureMapIn = torch.Tensor[\"batch\", \"in_channels\", \"in_height\", \"in_width\"]\n",
        "StackedFeatureMapOut = torch.Tensor[\"batch\", \"out_channels\", \"out_height\", \"out_width\"]\n",
        "def same_convolution_2d(x: StackedFeatureMapIn) -> StackedFeatureMapOut:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMC8Fw3zPSz"
      },
      "source": [
        "Here, \"map\" is meant to evoke space:\n",
        "our feature maps tell us where\n",
        "features are spatially located.\n",
        "\n",
        "An RGB image is a stacked feature map.\n",
        "It is composed of three feature maps.\n",
        "The first tells us where the \"red\" feature is present,\n",
        "the second \"green\", the third \"blue\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jIXT-mym3ljt"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png?20110219015028\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display.Image(\n",
        "    url=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png?20110219015028\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WfCcO5xJ-hG"
      },
      "source": [
        "When we apply a convolutional layer to a stacked feature map with some number of channels,\n",
        "we get back a stacked feature map with some number of channels.\n",
        "\n",
        "This output is also a stack of feature maps,\n",
        "and so it is a perfectly acceptable\n",
        "input to another convolutional layer.\n",
        "That means we can compose convolutional layers together,\n",
        "just as we composed generic linear layers together.\n",
        "We again weave non-linear functions in between our linear convolutions,\n",
        "creating a _convolutional neural network_, or CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R18TsGubJ_my"
      },
      "source": [
        "## Convolutional neural networks build up visual understanding layer by layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV03KmYBz2QM"
      },
      "source": [
        "What is the equivalent of the labels, red/green/blue,\n",
        "for the channels in these feature maps?\n",
        "What does a high activation in some position in channel 32\n",
        "of the fifteenth layer of my network tell me?\n",
        "\n",
        "There is no guaranteed way to automatically determine the answer,\n",
        "nor is there a guarantee that the result is human-interpretable.\n",
        "OpenAI's Clarity team spent several years \"reverse engineering\"\n",
        "state-of-the-art convolutiuonal neural networks trained on photographs\n",
        "and found that many of these channels are\n",
        "[directly interpretable](https://distill.pub/2018/building-blocks/).\n",
        "\n",
        "For example, they found that if they pass an image through\n",
        "[GoogLeNet](https://doi.org/10.1109/cvpr.2015.7298594),\n",
        "aka InceptionV1,\n",
        "the winner of the\n",
        "[2014 ImageNet Very Large Scale Visual Recognition Challenge](https://www.image-net.org/challenges/LSVRC/2014/),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "64KJR70q6dCh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a sample image\n",
        "display.Image(url=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7CvvG78CZ5"
      },
      "source": [
        "the features become increasingly complex,\n",
        "with channels in early layers (left)\n",
        "acting as maps for simple things like \"high frequency power\" or \"45 degree black-white edge\"\n",
        "and channels in later layers (to right)\n",
        "acting as feature maps for increasingly abstract concepts,\n",
        "like \"circle\" and eventually \"floppy round ear\" or \"pointy ear\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6w5_RR8d9jEY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\" width=\"1024\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from https://distill.pub/2018/building-blocks/\n",
        "display.Image(url=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\", width=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLiqEwMY_Co0"
      },
      "source": [
        "> <small> The small square images depict a heuristic estimate\n",
        "of what the entire collection of feature maps\n",
        "at a given layer represent (layer IDs at bottom).\n",
        "They are arranged in a spatial grid and their sizes represent\n",
        "the total magnitude of the layer's activations at that position.\n",
        "For details and interactivity, see\n",
        "[the original Distill article](https://distill.pub/2018/building-blocks/).</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl8XlEsaA54W"
      },
      "source": [
        "In the\n",
        "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
        "blogpost series,\n",
        "the Open AI Clarity team\n",
        "combines careful examination of weights\n",
        "with direct experimentation\n",
        "to build an understanding of how these higher-level features\n",
        "are constructed in GoogLeNet.\n",
        "\n",
        "For example,\n",
        "they are able to provide reasonable interpretations for\n",
        "[almost every channel in the first five layers](https://distill.pub/2020/circuits/early-vision/).\n",
        "\n",
        "The cell below will pull down their \"weight explorer\"\n",
        "and embed it in this notebook.\n",
        "By default, it starts on\n",
        "[the 52nd channel in the `conv2d1` layer](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d1_52.html),\n",
        "which constructs a large, phase-invariant\n",
        "[Gabor filter](https://en.wikipedia.org/wiki/Gabor_filter)\n",
        "from smaller, phase-sensitive filters.\n",
        "It is in turn used to construct\n",
        "[curve](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_180.html)\n",
        "and\n",
        "[texture](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_114.html)\n",
        "detectors --\n",
        "click on any image to navigate to the weight explorer page\n",
        "for that channel\n",
        "or change the `layer` and `idx`\n",
        "arguments.\n",
        "For additional context,\n",
        "check out the\n",
        "[Early Vision in InceptionV1 blogpost](https://distill.pub/2020/circuits/early-vision/).\n",
        "\n",
        "Click the \"View this neuron in the OpenAI Microscope\" link\n",
        "for an even richer interactive view,\n",
        "including activations on sample images\n",
        "([example](https://microscope.openai.com/models/inceptionv1/conv2d1_0/52)).\n",
        "\n",
        "The\n",
        "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
        "which this explorer accompanies\n",
        "is chock-full of empirical observations, theoretical speculation, and nuggets of wisdom\n",
        "that are invaluable for developing intuition about both\n",
        "convolutional networks in particular and visual perception in general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-hkYjdB-qQ"
      },
      "outputs": [],
      "source": [
        "layers = [\"conv2d0\", \"conv2d1\", \"conv2d2\", \"mixed3a\", \"mixed3b\"]\n",
        "layer = layers[1]\n",
        "idx = 52\n",
        "\n",
        "weight_explorer = display.IFrame(\n",
        "    src=f\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/{layer}_{idx}.html\", width=1024, height=720)\n",
        "weight_explorer.iframe = 'style=\"background: #FFF\";\\n><'.join(weight_explorer.iframe.split(\"><\"))  # inject background color\n",
        "weight_explorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ6_PCmVtTFH"
      },
      "source": [
        "# Applying convolutions to handwritten characters: `CNN`s on `EMNIST`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N--VkRtR5Yr-"
      },
      "source": [
        "If we load up the `CNN` class from `text_recognizer.models`,\n",
        "we'll see that a `data_config` is required to instantiate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3MA--zytTFH"
      },
      "outputs": [],
      "source": [
        "import text_recognizer.models\n",
        "\n",
        "\n",
        "text_recognizer.models.CNN??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yCP46PO6XDg"
      },
      "source": [
        "So before we can make our convolutional network and train it,\n",
        "we'll need to get a hold of some data.\n",
        "This isn't a general constraint by the way --\n",
        "it's an implementation detail of the `text_recognizer` library.\n",
        "But datasets and models are generally coupled,\n",
        "so it's common for them to share configuration information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z42K-jjtTFH"
      },
      "source": [
        "## The `EMNIST` Handwritten Character Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiifKuu4tTFH"
      },
      "source": [
        "We could just use `MNIST` here,\n",
        "as we did in\n",
        "[the first lab](https://fsdl.me/lab01-colab).\n",
        "\n",
        "But we're aiming to eventually build a handwritten text recognition system,\n",
        "which means we need to handle letters and punctuation,\n",
        "not just numbers.\n",
        "\n",
        "So we instead use _EMNIST_,\n",
        "or [Extended MNIST](https://paperswithcode.com/paper/emnist-an-extension-of-mnist-to-handwritten),\n",
        "which includes letters and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3ePZW1Tfa00K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMNIST dataset of handwritten characters and digits.\n",
            "\n",
            "    \"The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19\n",
            "    and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\"\n",
            "    From https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
            "\n",
            "    The data split we will use is\n",
            "    EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import text_recognizer.data\n",
        "\n",
        "\n",
        "emnist = text_recognizer.data.EMNIST()  # configure\n",
        "print(emnist.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_yjBYhla6qp"
      },
      "source": [
        "We've built a PyTorch Lightning `DataModule`\n",
        "to encapsulate all the code needed to get this dataset ready to go:\n",
        "downloading to disk,\n",
        "[reformatting to make loading faster](https://www.h5py.org/),\n",
        "and splitting into training, validation, and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty2vakBBtTFI"
      },
      "outputs": [],
      "source": [
        "emnist.prepare_data()  # download, save to disk\n",
        "emnist.setup()  # create torch.utils.data.Datasets, do train/val split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h9bAXcu8l5J"
      },
      "source": [
        "A brief aside: you might be wondering where this data goes.\n",
        "Datasets are saved to disk inside the repo folder,\n",
        "but not tracked in version control.\n",
        "`git` works well for versioning source code\n",
        "and other text files, but it's a poor fit for large binary data.\n",
        "We only track and version metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5cwDCM88SnU"
      },
      "outputs": [],
      "source": [
        "!echo {emnist.data_dirname()}\n",
        "!ls {emnist.data_dirname()}\n",
        "!ls {emnist.data_dirname() / \"raw\" / \"emnist\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdsIBL9MtTFI"
      },
      "source": [
        "This class comes with a pretty printing method\n",
        "for quick examination of some of that metadata and basic descriptive statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyw66d6GtTFI"
      },
      "outputs": [],
      "source": [
        "emnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0burlOLgoH"
      },
      "source": [
        "\n",
        "> <small> You can add pretty printing to your own Python classes by writing\n",
        "`__str__` or `__repr__` methods for them.\n",
        "The former is generally expected to be human-readable,\n",
        "while the latter is generally expected to be machine-readable;\n",
        "we've broken with that custom here and used `__repr__`. </small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJF3G5idtTFI"
      },
      "source": [
        "Because we've run `.prepare_data` and `.setup`,\n",
        "we can expect that this `DataModule` is ready to provide a `DataLoader`\n",
        "if we invoke the right method --\n",
        "sticking to the PyTorch Lightning API brings these kinds of convenient guarantees\n",
        "even when we're not using the `Trainer` class itself,\n",
        "[as described in Lab 2a](https://fsdl.me/lab02a-colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJghcZkWtTFI"
      },
      "outputs": [],
      "source": [
        "xs, ys = next(iter(emnist.train_dataloader()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FWjMT-tTFJ"
      },
      "source": [
        "Run the cell below to inspect random elements of this batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hywyEI_tTFJ"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "idx = random.randint(0, len(xs) - 1)\n",
        "\n",
        "print(emnist.mapping[ys[idx]])\n",
        "wandb.Image(xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdg_wYWntTFJ"
      },
      "source": [
        "## Putting convolutions in a `torch.nn.Module`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGuSx_zvtTFJ"
      },
      "source": [
        "Because we have the data,\n",
        "we now have a `data_config`\n",
        "and can instantiate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxLf7-5jtTFJ"
      },
      "outputs": [],
      "source": [
        "data_config = emnist.config()\n",
        "\n",
        "cnn = text_recognizer.models.CNN(data_config)\n",
        "cnn  # reveals the nn.Modules attached to our nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkeJNVnIMVzJ"
      },
      "source": [
        "We can run this network on our inputs,\n",
        "but we don't expect it to produce correct outputs without training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EwujOGqMAZY"
      },
      "outputs": [],
      "source": [
        "idx = random.randint(0, len(xs) - 1)\n",
        "outs = cnn(xs[idx:idx+1])\n",
        "\n",
        "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
        "wandb.Image(xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3L8u0estTFJ"
      },
      "source": [
        "We can inspect the `.forward` method to see how these `nn.Module`s are used.\n",
        "\n",
        "> <small> Note: we encourage you to read through the code --\n",
        "either inside the notebooks, as below,\n",
        "in your favorite text editor locally, or\n",
        "[on GitHub](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022-labs).\n",
        "There's lots of useful bits of Python that we don't have time to cover explicitly in the labs.</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RtA0W8jvtTFJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object `cnn.forward` not found.\n"
          ]
        }
      ],
      "source": [
        "cnn.forward??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCycQ88gtTFK"
      },
      "source": [
        "We apply convolutions followed by non-linearities,\n",
        "with intermittent \"pooling\" layers that apply downsampling --\n",
        "similar to the 1989\n",
        "[LeNet](https://doi.org/10.1162%2Fneco.1989.1.4.541)\n",
        "architecture or the 2012\n",
        "[AlexNet](https://doi.org/10.1145%2F3065386)\n",
        "architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkGJCnMttTFK"
      },
      "source": [
        "The final classification is performed by an MLP.\n",
        "\n",
        "In order to get vectors to pass into that MLP,\n",
        "we first apply `torch.flatten`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZPhw7ufAKZ7"
      },
      "outputs": [],
      "source": [
        "torch.flatten(torch.Tensor([[1, 2], [3, 4]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCoCa3vCNM8j"
      },
      "source": [
        "## Design considerations for CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDLEMnPINTj7"
      },
      "source": [
        "Since the release of AlexNet,\n",
        "there has been a feverish decade of engineering and innovation in CNNs --\n",
        "[dilated convolutions](https://arxiv.org/abs/1511.07122),\n",
        "[residual connections](https://arxiv.org/abs/1512.03385), and\n",
        "[batch normalization](https://arxiv.org/abs/1502.03167)\n",
        "came out in 2015 alone, and\n",
        "[work continues](https://arxiv.org/abs/2201.03545) --\n",
        "so we can only scratch the surface in this course and\n",
        "[the devil is in the details](https://arxiv.org/abs/1405.3531v4).\n",
        "\n",
        "The progress of DNNs in general and CNNs in particular\n",
        "has been mostly evolutionary,\n",
        "with lots of good ideas that didn't work out\n",
        "and weird hacks that stuck around because they did.\n",
        "That can make it very hard to design a fresh architecture\n",
        "from first principles that's anywhere near as effective as existing architectures.\n",
        "You're better off tweaking and mutating an existing architecture\n",
        "than trying to design one yourself.\n",
        "\n",
        "If you're not keeping close tabs on the field,\n",
        "when your first start looking for an architecture to base your work off of\n",
        "it's best to go to trusted aggregators, like\n",
        "[Torch IMage Models](https://github.com/rwightman/pytorch-image-models),\n",
        "or `timm`, on GitHub, or\n",
        "[Papers With Code](https://paperswithcode.com),\n",
        "specifically the section for\n",
        "[computer vision](https://paperswithcode.com/methods/area/computer-vision).\n",
        "You can also take a more bottom-up approach by checking\n",
        "the leaderboards of the latest\n",
        "[Kaggle competitions on computer vision](https://www.kaggle.com/competitions?searchQuery=computer+vision).\n",
        "\n",
        "We'll briefly touch here on some of the main design considerations\n",
        "with classic CNN architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd0OeyouDNlS"
      },
      "source": [
        "### Shapes and padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w3p8QP6AnGQ"
      },
      "source": [
        "In the `.forward` pass of the `CNN`,\n",
        "we've included comments that indicate the expected shapes\n",
        "of tensors after each line that changes the shape.\n",
        "\n",
        "Tracking and correctly handling shapes is one of the bugbears\n",
        "of CNNs, especially architectures,\n",
        "like LeNet/AlexNet, that include MLP components\n",
        "that can only operate on fixed-shape tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgbM30jstTFK"
      },
      "source": [
        "[Shape arithmetic gets pretty hairy pretty fast](https://arxiv.org/abs/1603.07285)\n",
        "if you're supporting the wide variety of convolutions.\n",
        "\n",
        "The easiest way to avoid shape bugs is to keep things simple:\n",
        "choose your convolution parameters,\n",
        "like `padding` and `stride`,\n",
        "to keep the shape the same before and after\n",
        "the convolution.\n",
        "\n",
        "That's what we do, by choosing `padding=1`\n",
        "for `kernel_size=3` and `stride=1`.\n",
        "With unit strides and odd-numbered kernel size,\n",
        "the padding that keeps\n",
        "the input the same size is `kernel_size // 2`.\n",
        "\n",
        "As shapes change, so does the amount of GPU memory taken up by the tensors.\n",
        "Keeping sizes fixed within a block removes one axis of variation\n",
        "in the demands on an important resource.\n",
        "\n",
        "After applying our pooling layer,\n",
        "we can just increase the number of kernels by the right factor\n",
        "to keep total tensor size,\n",
        "and thus memory footprint, constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCkTZGSDSBG"
      },
      "source": [
        "### Parameters, computation, and bottlenecks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZbgm7wztTFK"
      },
      "source": [
        "If we review the `num`ber of `el`ements in each of the layers,\n",
        "we see that one layer has far more entries than all the others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nfjPVwztTFK"
      },
      "outputs": [],
      "source": [
        "[p.numel() for p in cnn.parameters()]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzIoCz1FtTFK"
      },
      "source": [
        "The biggest layer is typically\n",
        "the one in between the convolutional component\n",
        "and the MLP component:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYrlUprltTFK"
      },
      "outputs": [],
      "source": [
        "biggest_layer = [p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())][0]\n",
        "biggest_layer.shape, cnn.fc_input_dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSHdvEGptTFL"
      },
      "source": [
        "This layer dominates the cost of storing the network on disk.\n",
        "That makes it a common target for\n",
        "regularization techniques like DropOut\n",
        "(as in our architecture)\n",
        "and performance optimizations like\n",
        "[pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html).\n",
        "\n",
        "Heuristically, we often associated more parameters with more computation.\n",
        "But just because that layer has the most parameters\n",
        "does not mean that most of the compute time is spent in that layer.\n",
        "\n",
        "Convolutions reuse the same parameters over and over,\n",
        "so the total number of FLOPs done by the layer can be higher\n",
        "than that done by layers with more parameters --\n",
        "much higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLisj1SptTFL"
      },
      "outputs": [],
      "source": [
        "# for the Linear layers, number of multiplications per input == nparams\n",
        "cnn.fc1.weight.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2oINHRtTFL"
      },
      "outputs": [],
      "source": [
        "# for the Conv2D layers, it's more complicated\n",
        "\n",
        "def approx_conv_multiplications(kernel_shape, input_size=(32, 28, 28)):  # this is a rough and dirty approximation\n",
        "    num_kernels, input_channels, kernel_height, kernel_width = kernel_shape\n",
        "    input_height, input_width = input_size[1], input_size[2]\n",
        "\n",
        "    multiplications_per_kernel_application = input_channels * kernel_height * kernel_width\n",
        "    num_applications = ((input_height - kernel_height + 1) * (input_width - kernel_width + 1))\n",
        "    mutliplications_per_kernel = num_applications * multiplications_per_kernel_application\n",
        "\n",
        "    return mutliplications_per_kernel * num_kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwCbZU9PtTFL"
      },
      "outputs": [],
      "source": [
        "approx_conv_multiplications(cnn.conv2.conv.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sdco4m9UtTFL"
      },
      "outputs": [],
      "source": [
        "# ratio of multiplications in the convolution to multiplications in the fully-connected layer is large!\n",
        "approx_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joVoBEtqtTFL"
      },
      "source": [
        "Depending on your compute hardware and the problem characteristics,\n",
        "either the MLP component or the convolutional component\n",
        "could become the critical bottleneck.\n",
        "\n",
        "When you're memory constrained, like when transferring a model \"over the wire\" to a browser,\n",
        "the MLP component is likely to be the bottleneck,\n",
        "whereas when you are compute-constrained, like when running a model on a low-power edge device\n",
        "or in an application with strict low-latency requirements,\n",
        "the convolutional component is likely to be the bottleneck.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGSyp67dtTFM"
      },
      "source": [
        "## Training a `CNN` on `EMNIST` with the Lightning `Trainer` and `run_experiment`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYTJs7snQfX0"
      },
      "source": [
        "We have a model and we have data,\n",
        "so we could just go ahead and start training in raw PyTorch,\n",
        "[as we did in Lab 01](https://fsdl.me/lab01-colab).\n",
        "\n",
        "But as we saw in that lab,\n",
        "there are good reasons to use a framework\n",
        "to organize training and provide fixed interfaces and abstractions.\n",
        "So we're going to use PyTorch Lightning, which is\n",
        "[covered in detail in Lab 02a](https://fsdl.me/lab02a-colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZYaJ4bdMcWc"
      },
      "source": [
        "We provide a simple script that implements a command line interface\n",
        "to training with PyTorch Lightning\n",
        "using the models and datasets in this repository:\n",
        "`training/run_experiment.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "52kIYhPBPLNZ"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'Trainer' has no attribute 'add_argparse_args'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "File \u001b[0;32m~/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/run_experiment.py:151\u001b[0m\n\u001b[1;32m    147\u001b[0m         trainer\u001b[39m.\u001b[39mtest(lit_model, datamodule\u001b[39m=\u001b[39mdata)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     main()\n",
            "File \u001b[0;32m~/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/run_experiment.py:99\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m     79\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m    Run an experiment.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     parser \u001b[39m=\u001b[39m _setup_parser()\n\u001b[1;32m    100\u001b[0m     args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n\u001b[1;32m    101\u001b[0m     data, model \u001b[39m=\u001b[39m setup_data_and_model_from_args(args)\n",
            "File \u001b[0;32m~/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/run_experiment.py:24\u001b[0m, in \u001b[0;36m_setup_parser\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m parser \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mArgumentParser(add_help\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Add Trainer specific arguments, such as --max_epochs, --gpus, --precision\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m trainer_parser \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer\u001b[39m.\u001b[39;49madd_argparse_args(parser)\n\u001b[1;32m     25\u001b[0m trainer_parser\u001b[39m.\u001b[39m_action_groups[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtitle \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTrainer Args\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m parser \u001b[39m=\u001b[39m argparse\u001b[39m.\u001b[39mArgumentParser(add_help\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, parents\u001b[39m=\u001b[39m[trainer_parser])\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Trainer' has no attribute 'add_argparse_args'"
          ]
        }
      ],
      "source": [
        "%run training/run_experiment.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkM_HpILSyC9"
      },
      "source": [
        "The `pl.Trainer` arguments come first\n",
        "and there\n",
        "[are a lot of them](https://pytorch-lightning.readthedocs.io/en/1.6.3/common/trainer.html),\n",
        "so if we want to see what's configurable for\n",
        "our `Model` or our `LitModel`,\n",
        "we want the last few dozen lines of the help message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G0dBhgogO8_A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/run_experiment.py\", line 10, in <module>\n",
            "    from text_recognizer import lit_models\n",
            "ModuleNotFoundError: No module named 'text_recognizer'\n"
          ]
        }
      ],
      "source": [
        "!python training/run_experiment.py --help --model_class CNN --data_class EMNIST  | tail -n 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCBQekrPRt90"
      },
      "source": [
        "The `run_experiment.py` file is also importable as a module,\n",
        "so that you can inspect its contents\n",
        "and play with its component functions in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CPumvYatPaiS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Run an experiment.\n",
            "\n",
            "    Sample command:\n",
            "    ```\n",
            "    python training/run_experiment.py --max_epochs=3 --gpus='0,' --num_workers=20 --model_class=MLP --data_class=MNIST\n",
            "    ```\n",
            "\n",
            "    For basic help documentation, run the command\n",
            "    ```\n",
            "    python training/run_experiment.py --help\n",
            "    ```\n",
            "\n",
            "    The available command line args differ depending on some of the arguments, including --model_class and --data_class.\n",
            "\n",
            "    To see which command line args are available and read their documentation, provide values for those arguments\n",
            "    before invoking --help, like so:\n",
            "    ```\n",
            "    python training/run_experiment.py --model_class=MLP --data_class=MNIST --help\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import training.run_experiment\n",
        "\n",
        "\n",
        "print(training.run_experiment.main.__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiZ3RwW2UzJm"
      },
      "source": [
        "Let's run training!\n",
        "\n",
        "Execute the cell below to launch a training job for a CNN on EMNIST with default arguments.\n",
        "\n",
        "This will take several minutes on commodity hardware,\n",
        "so feel free to keep reading while it runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RSJM5I2TSeG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "gpus = int(torch.cuda.is_available())  # use GPUs if they're available\n",
        "\n",
        "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ayQ4ByJOnnP"
      },
      "source": [
        "The first thing you'll see are a few logger messages from Lightning,\n",
        "then some info about the hardware you have available and are using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcMrZcecO1EF"
      },
      "source": [
        "Then you'll see a summary of your model,\n",
        "including module names, parameter counts,\n",
        "and information about model disk size.\n",
        "\n",
        "`torchmetrics` show up here as well,\n",
        "since they are also `nn.Module`s.\n",
        "See [Lab 02a](https://fsdl.me/lab02a-colab)\n",
        "for details.\n",
        "We're tracking accuracy on training, validation, and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twGp9iWOUSfc"
      },
      "source": [
        "You may also see a quick message in the terminal\n",
        "referencing a \"validation sanity check\".\n",
        "PyTorch Lightning runs a few batches of validation data\n",
        "through the model before the first training epoch.\n",
        "This helps prevent training runs from crashing\n",
        "at the end of the first epoch,\n",
        "which is otherwise the first time validation loops are triggered\n",
        "and is sometimes hours into training,\n",
        "by crashing them quickly at the start.\n",
        "\n",
        "If you want to turn off the check,\n",
        "use `--num_sanity_val_steps=0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnKN3_MiRpE4"
      },
      "source": [
        "Then, you'll see a bar indicating\n",
        "progress through the training epoch,\n",
        "alongside metrics like throughput and loss.\n",
        "\n",
        "When the first (and only) epoch ends,\n",
        "the model is run on the validation set\n",
        "and aggregate loss and accuracy are reported to the console."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2eMZz_HR8vV"
      },
      "source": [
        "At the end of training,\n",
        "we call `Trainer.test`\n",
        "to check performance on the test set.\n",
        "\n",
        "We typically see test accuracy around 75-80%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybpLiKBKSDXI"
      },
      "source": [
        "During training, PyTorch Lightning saves _checkpoints_\n",
        "(file extension `.ckpt`)\n",
        "that can be used to restart training.\n",
        "\n",
        "The final line output by `run_experiment`\n",
        "indicates where the model with the best performance\n",
        "on the validation set has been saved.\n",
        "\n",
        "The checkpointing behavior is configured using a\n",
        "[`ModelCheckpoint` callback](https://pytorch-lightning.readthedocs.io/en/1.6.3/api/pytorch_lightning.callbacks.ModelCheckpoint.html).\n",
        "The `run_experiment` script picks sensible defaults.\n",
        "\n",
        "These checkpoints contain the model weights.\n",
        "We can use them to los the model in the notebook and play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rqh9ZQsY8g4"
      },
      "outputs": [],
      "source": [
        "# we use a sequence of bash commands to get the latest checkpoint's filename\n",
        "#  by hand, you can just copy and paste it\n",
        "\n",
        "list_all_log_files = \"find training/logs/lightning_logs\"  # find avoids issues with \\n in filenames\n",
        "filter_to_ckpts = \"grep \\.ckpt$\"  # regex match on end of line\n",
        "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
        "take_first = \"head -n 1\"  # the first n elements, n=1\n",
        "\n",
        "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
        "latest_ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QW_CxR3coV6"
      },
      "source": [
        "To rebuild the model,\n",
        "we need to consider some implementation details of the `run_experiment` script.\n",
        "\n",
        "We use the parsed command line arguments, the `args`, to build the data and model,\n",
        "then use all three to build the `LightningModule`.\n",
        "\n",
        "Any `LightningModule` can be reinstantiated from a checkpoint\n",
        "using the `load_from_checkpoint` method,\n",
        "but we'll need to recreate and pass the `args`\n",
        "in order to reload the model.\n",
        "(We'll see how this can be automated later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVWEHcgvaSqZ"
      },
      "outputs": [],
      "source": [
        "import training.util\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "# if you change around model/data args in the command above, add them here\n",
        "#  tip: define the arguments as variables, like we've done for gpus\n",
        "#       and then add those variables to this dict so you don't need to\n",
        "#       remember to update/copy+paste\n",
        "\n",
        "args = Namespace(**{\n",
        "    \"model_class\": \"CNN\",\n",
        "    \"data_class\": \"EMNIST\"})\n",
        "\n",
        "\n",
        "_, cnn = training.util.setup_data_and_model_from_args(args)\n",
        "\n",
        "reloaded_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
        "   latest_ckpt, args=args, model=cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MynyI_eUcixa"
      },
      "source": [
        "With the model reloads, we can run it on some sample data\n",
        "and see how it's doing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L0HCxgVwcRAA"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'xs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb Cell 100\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y201sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(xs) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y201sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m outs \u001b[39m=\u001b[39m reloaded_model(xs[idx:idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y201sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput:\u001b[39m\u001b[39m\"\u001b[39m, emnist\u001b[39m.\u001b[39mmapping[torch\u001b[39m.\u001b[39margmax(outs)])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
          ]
        }
      ],
      "source": [
        "idx = random.randint(0, len(xs) - 1)\n",
        "outs = reloaded_model(xs[idx:idx+1])\n",
        "\n",
        "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
        "wandb.Image(xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6NtaHuVdfqt"
      },
      "source": [
        "I generally see subjectively good performance --\n",
        "without seeing the labels, I tend to agree with the model's output\n",
        "more often than the accuracy would suggest,\n",
        "since some classes, like c and C or o, O, and 0,\n",
        "are essentially indistinguishable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzcDcxpVkki"
      },
      "source": [
        "We can continue a promising training run from the checkpoint.\n",
        "Run the cell below to train the model just trained above\n",
        "for another epoch.\n",
        "Note that the training loss starts out close to where it ended\n",
        "in the previous run.\n",
        "\n",
        "Paired with cloud storage of checkpoints,\n",
        "this makes it possible to use\n",
        "[a cheaper type of cloud instance](https://cloud.google.com/blog/products/ai-machine-learning/reduce-the-costs-of-ml-workflows-with-preemptible-vms-and-gpus)\n",
        "that can be pre-empted by someone willing to pay more,\n",
        "which terminates your job.\n",
        "It's also helpful when using Google Colab for more serious projects --\n",
        "your training runs are no longer bound by the maximum uptime of a Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skqdikNtVnaf"
      },
      "outputs": [],
      "source": [
        "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
        "\n",
        "\n",
        "# and we can change the training hyperparameters, like batch size\n",
        "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus} \\\n",
        "  --batch_size 64 --load_checkpoint {latest_ckpt}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdNt6Z2tTFM"
      },
      "source": [
        "# Creating lines of text from handwritten characters: `EMNISTLines`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevtQpeDtTFM"
      },
      "source": [
        "We've got a training pipeline for our model and our data,\n",
        "and we can use that to make the loss go down\n",
        "and get better at the task.\n",
        "But the problem we're solving not obviously useful:\n",
        "the model is just learning how to handle\n",
        "centered, high-contrast, isolated characters.\n",
        "\n",
        "To make this work in a text recognition application,\n",
        "we would need a component to first pull out characters like that from images.\n",
        "That task is probably harder than the one we're currently learning.\n",
        "Plus, splitting into two separate components is against the ethos of deep learning,\n",
        "which operates \"end-to-end\".\n",
        "\n",
        "Let's kick the realism up one notch by building lines of text out of our characters:\n",
        "_synthesizing_ data for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH7i4JhWe7ch"
      },
      "source": [
        "Synthetic data is generally useful for augmenting limited real data.\n",
        "By construction we know the labels, since we created the data.\n",
        "Often, we can track covariates,\n",
        "like lighting features or subclass membership,\n",
        "that aren't always available in our labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQ_44TIe39m"
      },
      "source": [
        "To build fake handwriting,\n",
        "we'll combine two things:\n",
        "real handwritten letters and real text.\n",
        "\n",
        "We generate our fake text by drawing from the\n",
        "[Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "provided by the [`n`atural `l`anguage `t`ool`k`it](https://www.nltk.org/) library.\n",
        "\n",
        "First, we download that corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gtSg7Y8Ydxpa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/stanley/Documents/shidd\n",
            "[nltk_data]     er/ml/FullStackDL/fsdl-2022-\n",
            "[nltk_data]     labs/data/downloaded/nltk...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Generate text sentences using the Brown corpus.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from text_recognizer.data.sentence_generator import SentenceGenerator\n",
        "\n",
        "sentence_generator = SentenceGenerator()\n",
        "\n",
        "SentenceGenerator.__doc__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yal5eHk-aB4i"
      },
      "source": [
        "We can generate short snippets of text from the corpus with the `SentenceGenerator`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eRg_C1TYzwKX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "phase\n",
            "Within\n",
            "either\n",
            "the\n"
          ]
        }
      ],
      "source": [
        "print(*[sentence_generator.generate(max_length=16) for _ in range(4)], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGsBuMICaXnM"
      },
      "source": [
        "We use another `DataModule` to pick out the needed handwritten characters from `EMNIST`\n",
        "and glue them together into images containing the generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YtsGfSu6dpZ9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'EMNIST Lines dataset: synthetic handwriting lines dataset made from EMNIST characters.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emnist_lines = text_recognizer.data.EMNISTLines()  # configure\n",
        "emnist_lines.__doc__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dik_SyEdb0st"
      },
      "source": [
        "This can take several minutes when first run,\n",
        "but afterwards data is persisted to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SofIYHOUtTFM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMNISTLinesDataset generating data for train...\n",
            "EMNISTLinesDataset generating data for val...\n",
            "EMNISTLinesDataset generating data for test...\n",
            "EMNISTLinesDataset loading data from HDF5...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EMNIST Lines Dataset\n",
              "Min overlap: 0\n",
              "Max overlap: 0.33\n",
              "Num classes: 83\n",
              "Dims: (1, 28, 896)\n",
              "Output dims: (32, 1)\n",
              "Train/val/test sizes: 10000, 2000, 2000\n",
              "Batch x stats: (torch.Size([128, 1, 28, 896]), torch.float32, 0.0, 0.07314881682395935, 0.23142141103744507, 1.0)\n",
              "Batch y stats: (torch.Size([128, 32]), torch.int64, 3, 66)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emnist_lines.prepare_data()  # download, save to disk\n",
        "emnist_lines.setup()  # create torch.utils.data.Datasets, do train/val split\n",
        "emnist_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axESuV1SeoM6"
      },
      "source": [
        "Again, we're using the `LightningDataModule` interface\n",
        "to organize our data prep,\n",
        "so we can now fetch a batch and take a look at some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1J7f2I9ggBi-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 1, 28, 896]), torch.Size([128, 32]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "line_xs, line_ys = next(iter(emnist_lines.val_dataloader()))\n",
        "line_xs.shape, line_ys.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "B0yHgbW2gHgP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i-n- -t-h-e- -n-a-m-e- -o-f- -t-h-e- -L-o-r-d-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>-<P>\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb Cell 117\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(line_xs) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y224sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(read_line_labels(line_ys[idx])))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/lab02b_cnn.ipynb#Y224sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m wandb\u001b[39m.\u001b[39mImage(line_xs[idx])\u001b[39m.\u001b[39mimage\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ],
      "source": [
        "def read_line_labels(labels):\n",
        "    return [emnist_lines.mapping[label] for label in labels]\n",
        "\n",
        "idx = random.randint(0, len(line_xs) - 1)\n",
        "\n",
        "print(\"-\".join(read_line_labels(line_ys[idx])))\n",
        "wandb.Image(line_xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xirEmNPNtTFM"
      },
      "source": [
        "The result looks\n",
        "[kind of like a ransom note](https://tvtropes.org/pmwiki/pmwiki.php/Main/CutAndPasteNote)\n",
        "and is not yet anywhere near realistic, even for single lines --\n",
        "letters don't overlap, the exact same handwritten letter is repeated\n",
        "if the character appears more than once in the snippet --\n",
        "but it's a start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRWbSzkotTFM"
      },
      "source": [
        "# Applying CNNs to handwritten text: `LineCNNSimple`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzwYBv82tTFM"
      },
      "source": [
        "The `LineCNNSimple` class builds on the `CNN` class and can be applied to this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqeImjd2lF7p"
      },
      "outputs": [],
      "source": [
        "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
        "line_cnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi6g0acoxJO4"
      },
      "source": [
        "The `nn.Module`s look much the same,\n",
        "but the way they are used is different,\n",
        "which we can see by examining the `.forward` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg3UJhibxHfC"
      },
      "outputs": [],
      "source": [
        "line_cnn.forward??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAW7EWVlxMhd"
      },
      "source": [
        "The `CNN`, which operates on square images,\n",
        "is applied to our wide image repeatedly,\n",
        "slid over by the `W`indow `S`ize each time.\n",
        "We effectively convolve the network with the input image.\n",
        "\n",
        "Like our synthetic data, it is crude\n",
        "but it's enough to get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4J13yLisiC"
      },
      "outputs": [],
      "source": [
        "idx = random.randint(0, len(line_xs) - 1)\n",
        "\n",
        "outs, = line_cnn(line_xs[idx:idx+1])\n",
        "preds = torch.argmax(outs, 0)\n",
        "\n",
        "print(\"-\".join(read_line_labels(preds)))\n",
        "wandb.Image(line_xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxHI4Gzndbxg"
      },
      "source": [
        "> <small> You may notice that this randomly-initialized\n",
        "network tends to predict some characters far more often than others,\n",
        "rather than predicting all characters with equal likelihood.\n",
        "This is a commonly-observed phenomenon in deep networks.\n",
        "It is connected to issues with\n",
        "[model calibration](https://arxiv.org/abs/1706.04599)\n",
        "and Bayesian uses of DNNs\n",
        "(see e.g. Figure 7 of\n",
        "[Wenzel et al. 2020](https://arxiv.org/abs/2002.02405)).</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSonI9KcfJrB"
      },
      "source": [
        "Let's launch a training run with the default parameters.\n",
        "\n",
        "This cell should run in just a few minutes on typical hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rsbJdeRiwSVA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMNISTLinesDataset loading data from HDF5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type          | Params\n",
            "--------------------------------------------\n",
            "0 | model     | LineCNNSimple | 1.7 M \n",
            "1 | model.cnn | CNN           | 1.7 M \n",
            "2 | train_acc | Accuracy      | 0     \n",
            "3 | val_acc   | Accuracy      | 0     \n",
            "4 | test_acc  | Accuracy      | 0     \n",
            "--------------------------------------------\n",
            "1.7 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.7 M     Total params\n",
            "6.616     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbdf990917ce4c7b80a8b2dba693243d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9d28ef8e5a74b45826d631674db5eea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baa52bbe656c41ec9cba00a4c210799f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7407f1d3def94291b8ad38ac8d1073f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Best model saved at: /home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/logs/lightning_logs/version_2/epoch=0001-validation.loss=1.381.ckpt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EMNISTLinesDataset loading data from HDF5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Restoring states from the checkpoint path at /home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/logs/lightning_logs/version_2/epoch=0001-validation.loss=1.381.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loaded model weights from checkpoint at /home/stanley/Documents/shidder/ml/FullStackDL/fsdl-2022-labs/lab02/training/logs/lightning_logs/version_2/epoch=0001-validation.loss=1.381.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b567e2986762440ebcafab3ebcadcb44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "        test/acc                0.67578125\n",
            "        test/loss           1.4334373474121094\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "%run training/run_experiment.py --model_class LineCNNSimple --data_class EMNISTLines \\\n",
        "  --batch_size 32 --gpus 1 --max_epochs 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9e5nTplfoXG"
      },
      "source": [
        "You should see a test accuracy in the 65-70% range.\n",
        "\n",
        "That seems pretty good,\n",
        "especially for a simple model trained in a minute.\n",
        "\n",
        "Let's reload the model and run it on some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NuXazAvw9NA"
      },
      "outputs": [],
      "source": [
        "# if you change around model/data args in the command above, add them here\n",
        "#  tip: define the arguments as variables, like we've done for gpus\n",
        "#       and then add those variables to this dict so you don't need to\n",
        "#       remember to update/copy+paste\n",
        "\n",
        "args = Namespace(**{\n",
        "    \"model_class\": \"LineCNNSimple\",\n",
        "    \"data_class\": \"EMNISTLines\"})\n",
        "\n",
        "\n",
        "_, line_cnn = training.util.setup_data_and_model_from_args(args)\n",
        "\n",
        "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
        "print(latest_ckpt)\n",
        "\n",
        "reloaded_lines_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
        "   latest_ckpt, args=args, model=line_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ziVROkxkGC"
      },
      "outputs": [],
      "source": [
        "idx = random.randint(0, len(line_xs) - 1)\n",
        "\n",
        "outs, = reloaded_lines_model(line_xs[idx:idx+1])\n",
        "preds = torch.argmax(outs, 0)\n",
        "\n",
        "print(\"-\".join(read_line_labels(preds)))\n",
        "wandb.Image(line_xs[idx]).image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9bQCHtYgA0S"
      },
      "source": [
        "In general,\n",
        "we see predictions that have very low subjective quality:\n",
        "it seems like most of the letters are wrong\n",
        "and the model often prefers to predict the most common letters\n",
        "in the dataset, like `e`.\n",
        "\n",
        "Notice, however, that many of the\n",
        "characters in a given line are padding characters, `<P>`.\n",
        "\n",
        "A model that always predicts `<P>` can achieve around 50% accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE-T7zgDgo7-"
      },
      "outputs": [],
      "source": [
        "padding_token = emnist_lines.emnist.inverse_mapping[\"<P>\"]\n",
        "torch.sum(line_ys == padding_token) / line_ys.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGHWmOyVh5rV"
      },
      "source": [
        "There are ways to adjust your classification metrics to\n",
        "[handle this particular issue](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall).\n",
        "In general it's good to find a metric\n",
        "that has baseline performance at 0 and perfect performance at 1,\n",
        "so that numbers are clearly interpretable.\n",
        "\n",
        "But it's an important reminder to actually look\n",
        "at your model's behavior from time to time.\n",
        "Metrics are single numbers,\n",
        "so they by necessity throw away a ton of information\n",
        "about your model's behavior,\n",
        "some of which is deeply relevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p--KWZ9YJWQ"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srQnoOK8YLDv"
      },
      "source": [
        "### 🌟 Research a `pl.Trainer` argument and try it out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j652MtkYR8n"
      },
      "source": [
        "The Lightning `Trainer` class is highly configurable\n",
        "and has accumulated a number of features as Lightning has matured.\n",
        "\n",
        "Check out the documentation for this class\n",
        "and pick an argument to try out with `training/run_experiment.py`.\n",
        "Look for edge cases in its behavior,\n",
        "especially when combined with other arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UWNicq_jS7k"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "pl_version = pl.__version__\n",
        "\n",
        "print(\"pl.Trainer guide URL:\", f\"https://pytorch-lightning.readthedocs.io/en/{pl_version}/common/trainer.html\")\n",
        "print(\"pl.Trainer reference docs URL:\", f\"https://pytorch-lightning.readthedocs.io/en/{pl_version}/api/pytorch_lightning.trainer.trainer.Trainer.html\")\n",
        "\n",
        "pl.Trainer??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "14AOfjqqYOoT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: run_experiment.py [--logger [LOGGER]]\n",
            "                         [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
            "                         [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
            "                         [--default_root_dir DEFAULT_ROOT_DIR]\n",
            "                         [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
            "                         [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
            "                         [--process_position PROCESS_POSITION]\n",
            "                         [--num_nodes NUM_NODES]\n",
            "                         [--num_processes NUM_PROCESSES] [--devices DEVICES]\n",
            "                         [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
            "                         [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
            "                         [--log_gpu_memory LOG_GPU_MEMORY]\n",
            "                         [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
            "                         [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
            "                         [--overfit_batches OVERFIT_BATCHES]\n",
            "                         [--track_grad_norm TRACK_GRAD_NORM]\n",
            "                         [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
            "                         [--fast_dev_run [FAST_DEV_RUN]]\n",
            "                         [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
            "                         [--max_epochs MAX_EPOCHS] [--min_epochs MIN_EPOCHS]\n",
            "                         [--max_steps MAX_STEPS] [--min_steps MIN_STEPS]\n",
            "                         [--max_time MAX_TIME]\n",
            "                         [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
            "                         [--limit_val_batches LIMIT_VAL_BATCHES]\n",
            "                         [--limit_test_batches LIMIT_TEST_BATCHES]\n",
            "                         [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
            "                         [--val_check_interval VAL_CHECK_INTERVAL]\n",
            "                         [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
            "                         [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
            "                         [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
            "                         [--sync_batchnorm [SYNC_BATCHNORM]]\n",
            "                         [--precision PRECISION]\n",
            "                         [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
            "                         [--weights_summary WEIGHTS_SUMMARY]\n",
            "                         [--weights_save_path WEIGHTS_SAVE_PATH]\n",
            "                         [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
            "                         [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                         [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
            "                         [--deterministic [DETERMINISTIC]]\n",
            "                         [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
            "                         [--auto_lr_find [AUTO_LR_FIND]]\n",
            "                         [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
            "                         [--detect_anomaly [DETECT_ANOMALY]]\n",
            "                         [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
            "                         [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
            "                         [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
            "                         [--amp_level AMP_LEVEL]\n",
            "                         [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
            "                         [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
            "                         [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
            "                         [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
            "                         [--data_class DATA_CLASS] [--model_class MODEL_CLASS]\n",
            "                         [--load_checkpoint LOAD_CHECKPOINT]\n",
            "                         [--stop_early STOP_EARLY] [--batch_size BATCH_SIZE]\n",
            "                         [--num_workers NUM_WORKERS] [--fc1 FC1] [--fc2 FC2]\n",
            "                         [--fc_dropout FC_DROPOUT] [--optimizer OPTIMIZER]\n",
            "                         [--lr LR] [--one_cycle_max_lr ONE_CYCLE_MAX_LR]\n",
            "                         [--one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS]\n",
            "                         [--loss LOSS] [--help]\n",
            "\n",
            "options:\n",
            "  --data_class DATA_CLASS\n",
            "                        String identifier for the data class, relative to\n",
            "                        text_recognizer.data.\n",
            "  --model_class MODEL_CLASS\n",
            "                        String identifier for the model class, relative to\n",
            "                        text_recognizer.models.\n",
            "  --load_checkpoint LOAD_CHECKPOINT\n",
            "                        If passed, loads a model from the provided path.\n",
            "  --stop_early STOP_EARLY\n",
            "                        If non-zero, applies early stopping, with the provided\n",
            "                        value as the 'patience' argument. Default is 0.\n",
            "  --help, -h\n",
            "\n",
            "pl.Trainer:\n",
            "  --logger [LOGGER]     Logger (or iterable collection of loggers) for\n",
            "                        experiment tracking. A ``True`` value uses the default\n",
            "                        ``TensorBoardLogger``. ``False`` will disable logging.\n",
            "                        If multiple loggers are provided and the `save_dir`\n",
            "                        property of that logger is not set, local files\n",
            "                        (checkpoints, profiler traces, etc.) are saved in\n",
            "                        ``default_root_dir`` rather than in the ``log_dir`` of\n",
            "                        any of the individual loggers. Default: ``True``.\n",
            "  --checkpoint_callback [CHECKPOINT_CALLBACK]\n",
            "                        If ``True``, enable checkpointing. Default: ``None``.\n",
            "                        .. deprecated:: v1.5 ``checkpoint_callback`` has been\n",
            "                        deprecated in v1.5 and will be removed in v1.7. Please\n",
            "                        consider using ``enable_checkpointing`` instead.\n",
            "  --enable_checkpointing [ENABLE_CHECKPOINTING]\n",
            "                        If ``True``, enable checkpointing. It will configure a\n",
            "                        default ModelCheckpoint callback if there is no user-\n",
            "                        defined ModelCheckpoint in :paramref:`~pytorch_lightni\n",
            "                        ng.trainer.trainer.Trainer.callbacks`. Default:\n",
            "                        ``True``.\n",
            "  --default_root_dir DEFAULT_ROOT_DIR\n",
            "                        Default path for logs and weights when no\n",
            "                        logger/ckpt_callback passed. Default: ``os.getcwd()``.\n",
            "                        Can be remote file paths such as `s3://mybucket/path`\n",
            "                        or 'hdfs://path/'\n",
            "  --gradient_clip_val GRADIENT_CLIP_VAL\n",
            "                        The value at which to clip gradients. Passing\n",
            "                        ``gradient_clip_val=None`` disables gradient clipping.\n",
            "                        If using Automatic Mixed Precision (AMP), the\n",
            "                        gradients will be unscaled before. Default: ``None``.\n",
            "  --gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM\n",
            "                        The gradient clipping algorithm to use. Pass\n",
            "                        ``gradient_clip_algorithm=\"value\"`` to clip by value,\n",
            "                        and ``gradient_clip_algorithm=\"norm\"`` to clip by\n",
            "                        norm. By default it will be set to ``\"norm\"``.\n",
            "  --process_position PROCESS_POSITION\n",
            "                        Orders the progress bar when running multiple models\n",
            "                        on same machine. .. deprecated:: v1.5\n",
            "                        ``process_position`` has been deprecated in v1.5 and\n",
            "                        will be removed in v1.7. Please pass :class:`~pytorch_\n",
            "                        lightning.callbacks.progress.TQDMProgressBar` with\n",
            "                        ``process_position`` directly to the Trainer's\n",
            "                        ``callbacks`` argument instead.\n",
            "  --num_nodes NUM_NODES\n",
            "                        Number of GPU nodes for distributed training. Default:\n",
            "                        ``1``.\n",
            "  --num_processes NUM_PROCESSES\n",
            "                        Number of processes for distributed training with\n",
            "                        ``accelerator=\"cpu\"``. Default: ``1``.\n",
            "  --devices DEVICES     Will be mapped to either `gpus`, `tpu_cores`,\n",
            "                        `num_processes` or `ipus`, based on the accelerator\n",
            "                        type.\n",
            "  --gpus GPUS           Number of GPUs to train on (int) or which GPUs to\n",
            "                        train on (list or str) applied per node Default:\n",
            "                        ``None``.\n",
            "  --auto_select_gpus [AUTO_SELECT_GPUS]\n",
            "                        If enabled and ``gpus`` or ``devices`` is an integer,\n",
            "                        pick available gpus automatically. This is especially\n",
            "                        useful when GPUs are configured to be in \"exclusive\n",
            "                        mode\", such that only one process at a time can access\n",
            "                        them. Default: ``False``.\n",
            "  --tpu_cores TPU_CORES\n",
            "                        How many TPU cores to train on (1 or 8) / Single TPU\n",
            "                        to train on (1) Default: ``None``.\n",
            "  --ipus IPUS           How many IPUs to train on. Default: ``None``.\n",
            "  --log_gpu_memory LOG_GPU_MEMORY\n",
            "                        None, 'min_max', 'all'. Might slow performance. ..\n",
            "                        deprecated:: v1.5 Deprecated in v1.5.0 and will be\n",
            "                        removed in v1.7.0 Please use the\n",
            "                        ``DeviceStatsMonitor`` callback directly instead.\n",
            "  --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE\n",
            "                        How often to refresh progress bar (in steps). Value\n",
            "                        ``0`` disables progress bar. Ignored when a custom\n",
            "                        progress bar is passed to\n",
            "                        :paramref:`~Trainer.callbacks`. Default: None, means a\n",
            "                        suitable value will be chosen based on the environment\n",
            "                        (terminal, Google COLAB, etc.). .. deprecated:: v1.5\n",
            "                        ``progress_bar_refresh_rate`` has been deprecated in\n",
            "                        v1.5 and will be removed in v1.7. Please pass :class:`\n",
            "                        ~pytorch_lightning.callbacks.progress.TQDMProgressBar`\n",
            "                        with ``refresh_rate`` directly to the Trainer's\n",
            "                        ``callbacks`` argument instead. To disable the\n",
            "                        progress bar, pass ``enable_progress_bar = False`` to\n",
            "                        the Trainer.\n",
            "  --enable_progress_bar [ENABLE_PROGRESS_BAR]\n",
            "                        Whether to enable to progress bar by default. Default:\n",
            "                        ``False``.\n",
            "  --overfit_batches OVERFIT_BATCHES\n",
            "                        Overfit a fraction of training data (float) or a set\n",
            "                        number of batches (int). Default: ``0.0``.\n",
            "  --track_grad_norm TRACK_GRAD_NORM\n",
            "                        -1 no tracking. Otherwise tracks that p-norm. May be\n",
            "                        set to 'inf' infinity-norm. If using Automatic Mixed\n",
            "                        Precision (AMP), the gradients will be unscaled before\n",
            "                        logging them. Default: ``-1``.\n",
            "  --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH\n",
            "                        Check val every n train epochs. Default: ``1``.\n",
            "  --fast_dev_run [FAST_DEV_RUN]\n",
            "                        Runs n if set to ``n`` (int) else 1 if set to ``True``\n",
            "                        batch(es) of train, val and test to find any bugs (ie:\n",
            "                        a sort of unit test). Default: ``False``.\n",
            "  --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES\n",
            "                        Accumulates grads every k batches or as set up in the\n",
            "                        dict. Default: ``None``.\n",
            "  --max_epochs MAX_EPOCHS\n",
            "                        Stop training once this number of epochs is reached.\n",
            "                        Disabled by default (None). If both max_epochs and\n",
            "                        max_steps are not specified, defaults to ``max_epochs\n",
            "                        = 1000``. To enable infinite training, set\n",
            "                        ``max_epochs = -1``.\n",
            "  --min_epochs MIN_EPOCHS\n",
            "                        Force training for at least these many epochs.\n",
            "                        Disabled by default (None).\n",
            "  --max_steps MAX_STEPS\n",
            "                        Stop training after this number of steps. Disabled by\n",
            "                        default (-1). If ``max_steps = -1`` and ``max_epochs =\n",
            "                        None``, will default to ``max_epochs = 1000``. To\n",
            "                        enable infinite training, set ``max_epochs`` to\n",
            "                        ``-1``.\n",
            "  --min_steps MIN_STEPS\n",
            "                        Force training for at least these number of steps.\n",
            "                        Disabled by default (``None``).\n",
            "  --max_time MAX_TIME   Stop training after this amount of time has passed.\n",
            "                        Disabled by default (``None``). The time duration can\n",
            "                        be specified in the format DD:HH:MM:SS (days, hours,\n",
            "                        minutes seconds), as a :class:`datetime.timedelta`, or\n",
            "                        a dictionary with keys that will be passed to\n",
            "                        :class:`datetime.timedelta`.\n",
            "  --limit_train_batches LIMIT_TRAIN_BATCHES\n",
            "                        How much of training dataset to check (float =\n",
            "                        fraction, int = num_batches). Default: ``1.0``.\n",
            "  --limit_val_batches LIMIT_VAL_BATCHES\n",
            "                        How much of validation dataset to check (float =\n",
            "                        fraction, int = num_batches). Default: ``1.0``.\n",
            "  --limit_test_batches LIMIT_TEST_BATCHES\n",
            "                        How much of test dataset to check (float = fraction,\n",
            "                        int = num_batches). Default: ``1.0``.\n",
            "  --limit_predict_batches LIMIT_PREDICT_BATCHES\n",
            "                        How much of prediction dataset to check (float =\n",
            "                        fraction, int = num_batches). Default: ``1.0``.\n",
            "  --val_check_interval VAL_CHECK_INTERVAL\n",
            "                        How often to check the validation set. Pass a\n",
            "                        ``float`` in the range [0.0, 1.0] to check after a\n",
            "                        fraction of the training epoch. Pass an ``int`` to\n",
            "                        check after a fixed number of training batches.\n",
            "                        Default: ``1.0``.\n",
            "  --flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS\n",
            "                        How often to flush logs to disk (defaults to every 100\n",
            "                        steps). .. deprecated:: v1.5\n",
            "                        ``flush_logs_every_n_steps`` has been deprecated in\n",
            "                        v1.5 and will be removed in v1.7. Please configure\n",
            "                        flushing directly in the logger instead.\n",
            "  --log_every_n_steps LOG_EVERY_N_STEPS\n",
            "                        How often to log within steps. Default: ``50``.\n",
            "  --accelerator ACCELERATOR\n",
            "                        Supports passing different accelerator types (\"cpu\",\n",
            "                        \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\") as well as custom\n",
            "                        accelerator instances. .. deprecated:: v1.5 Passing\n",
            "                        training strategies (e.g., 'ddp') to ``accelerator``\n",
            "                        has been deprecated in v1.5.0 and will be removed in\n",
            "                        v1.7.0. Please use the ``strategy`` argument instead.\n",
            "  --strategy STRATEGY   Supports different training strategies with aliases as\n",
            "                        well custom strategies. Default: ``None``.\n",
            "  --sync_batchnorm [SYNC_BATCHNORM]\n",
            "                        Synchronize batch norm layers between process\n",
            "                        groups/whole world. Default: ``False``.\n",
            "  --precision PRECISION\n",
            "                        Double precision (64), full precision (32), half\n",
            "                        precision (16) or bfloat16 precision (bf16). Can be\n",
            "                        used on CPU, GPU, TPUs, HPUs or IPUs. Default: ``32``.\n",
            "  --enable_model_summary [ENABLE_MODEL_SUMMARY]\n",
            "                        Whether to enable model summarization by default.\n",
            "                        Default: ``True``.\n",
            "  --weights_summary WEIGHTS_SUMMARY\n",
            "                        Prints a summary of the weights when training begins.\n",
            "                        .. deprecated:: v1.5 ``weights_summary`` has been\n",
            "                        deprecated in v1.5 and will be removed in v1.7. To\n",
            "                        disable the summary, pass ``enable_model_summary =\n",
            "                        False`` to the Trainer. To customize the summary, pass\n",
            "                        :class:`~pytorch_lightning.callbacks.model_summary.Mod\n",
            "                        elSummary` directly to the Trainer's ``callbacks``\n",
            "                        argument.\n",
            "  --weights_save_path WEIGHTS_SAVE_PATH\n",
            "                        Where to save weights if specified. Will override\n",
            "                        default_root_dir for checkpoints only. Use this if for\n",
            "                        whatever reason you need the checkpoints stored in a\n",
            "                        different place than the logs written in\n",
            "                        `default_root_dir`. Can be remote file paths such as\n",
            "                        `s3://mybucket/path` or 'hdfs://path/' Defaults to\n",
            "                        `default_root_dir`. .. deprecated:: v1.6\n",
            "                        ``weights_save_path`` has been deprecated in v1.6 and\n",
            "                        will be removed in v1.8. Please pass ``dirpath``\n",
            "                        directly to the :class:`~pytorch_lightning.callbacks.m\n",
            "                        odel_checkpoint.ModelCheckpoint` callback.\n",
            "  --num_sanity_val_steps NUM_SANITY_VAL_STEPS\n",
            "                        Sanity check runs n validation batches before starting\n",
            "                        the training routine. Set it to `-1` to run all\n",
            "                        batches in all validation dataloaders. Default: ``2``.\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path/URL of the checkpoint from which training is\n",
            "                        resumed. If there is no checkpoint file at the path,\n",
            "                        an exception is raised. If resuming from mid-epoch\n",
            "                        checkpoint, training will start from the beginning of\n",
            "                        the next epoch. .. deprecated:: v1.5\n",
            "                        ``resume_from_checkpoint`` is deprecated in v1.5 and\n",
            "                        will be removed in v2.0. Please pass the path to\n",
            "                        ``Trainer.fit(..., ckpt_path=...)`` instead.\n",
            "  --profiler PROFILER   To profile individual steps during training and assist\n",
            "                        in identifying bottlenecks. Default: ``None``.\n",
            "  --benchmark [BENCHMARK]\n",
            "                        Sets ``torch.backends.cudnn.benchmark``. Defaults to\n",
            "                        ``True`` if :paramref:`~pytorch_lightning.trainer.trai\n",
            "                        ner.Trainer.deterministic` is ``False``. Overwrite to\n",
            "                        manually set a different value. Default: ``None``.\n",
            "  --deterministic [DETERMINISTIC]\n",
            "                        If ``True``, sets whether PyTorch operations must use\n",
            "                        deterministic algorithms. Default: ``False``.\n",
            "  --reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS\n",
            "                        Set to a non-negative integer to reload dataloaders\n",
            "                        every n epochs. Default: ``0``.\n",
            "  --auto_lr_find [AUTO_LR_FIND]\n",
            "                        If set to True, will make trainer.tune() run a\n",
            "                        learning rate finder, trying to optimize initial\n",
            "                        learning for faster convergence. trainer.tune() method\n",
            "                        will set the suggested learning rate in self.lr or\n",
            "                        self.learning_rate in the LightningModule. To use a\n",
            "                        different key set a string instead of True with the\n",
            "                        key name. Default: ``False``.\n",
            "  --replace_sampler_ddp [REPLACE_SAMPLER_DDP]\n",
            "                        Explicitly enables or disables sampler replacement. If\n",
            "                        not specified this will toggled automatically when DDP\n",
            "                        is used. By default it will add ``shuffle=True`` for\n",
            "                        train sampler and ``shuffle=False`` for val/test\n",
            "                        sampler. If you want to customize it, you can set\n",
            "                        ``replace_sampler_ddp=False`` and add your own\n",
            "                        distributed sampler.\n",
            "  --detect_anomaly [DETECT_ANOMALY]\n",
            "                        Enable anomaly detection for the autograd engine.\n",
            "                        Default: ``False``.\n",
            "  --auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]\n",
            "                        If set to True, will `initially` run a batch size\n",
            "                        finder trying to find the largest batch size that fits\n",
            "                        into memory. The result will be stored in\n",
            "                        self.batch_size in the LightningModule. Additionally,\n",
            "                        can be set to either `power` that estimates the batch\n",
            "                        size through a power search or `binsearch` that\n",
            "                        estimates the batch size through a binary search.\n",
            "                        Default: ``False``.\n",
            "  --prepare_data_per_node [PREPARE_DATA_PER_NODE]\n",
            "                        If True, each LOCAL_RANK=0 will call prepare data.\n",
            "                        Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare\n",
            "                        data .. deprecated:: v1.5 Deprecated in v1.5.0 and\n",
            "                        will be removed in v1.7.0 Please set\n",
            "                        ``prepare_data_per_node`` in ``LightningDataModule``\n",
            "                        and/or ``LightningModule`` directly instead.\n",
            "  --plugins PLUGINS     Plugins allow modification of core behavior like ddp\n",
            "                        and amp, and enable custom lightning plugins. Default:\n",
            "                        ``None``.\n",
            "  --amp_backend AMP_BACKEND\n",
            "                        The mixed precision backend to use (\"native\" or\n",
            "                        \"apex\"). Default: ``'native''``.\n",
            "  --amp_level AMP_LEVEL\n",
            "                        The optimization level to use (O1, O2, etc...). By\n",
            "                        default it will be set to \"O2\" if ``amp_backend`` is\n",
            "                        set to \"apex\".\n",
            "  --move_metrics_to_cpu [MOVE_METRICS_TO_CPU]\n",
            "                        Whether to force internal logged metrics to be moved\n",
            "                        to cpu. This can save some gpu memory, but can make\n",
            "                        training slower. Use with attention. Default:\n",
            "                        ``False``.\n",
            "  --multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE\n",
            "                        How to loop over the datasets when there are multiple\n",
            "                        train loaders. In 'max_size_cycle' mode, the trainer\n",
            "                        ends one epoch when the largest dataset is traversed,\n",
            "                        and smaller datasets reload when running out of their\n",
            "                        data. In 'min_size' mode, all the datasets reload when\n",
            "                        reaching the minimum length of datasets. Default:\n",
            "                        ``\"max_size_cycle\"``.\n",
            "  --stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]\n",
            "                        Whether to use `Stochastic Weight Averaging (SWA)\n",
            "                        <https://pytorch.org/blog/pytorch-1.6-now-includes-\n",
            "                        stochastic-weight-averaging/>`_. Default: ``False``.\n",
            "                        .. deprecated:: v1.5 ``stochastic_weight_avg`` has\n",
            "                        been deprecated in v1.5 and will be removed in v1.7.\n",
            "                        Please pass :class:`~pytorch_lightning.callbacks.stoch\n",
            "                        astic_weight_avg.StochasticWeightAveraging` directly\n",
            "                        to the Trainer's ``callbacks`` argument instead.\n",
            "  --terminate_on_nan [TERMINATE_ON_NAN]\n",
            "                        If set to True, will terminate training (by raising a\n",
            "                        `ValueError`) at the end of each training batch, if\n",
            "                        any of the parameters or the loss are NaN or +/-inf.\n",
            "                        .. deprecated:: v1.5 Trainer argument\n",
            "                        ``terminate_on_nan`` was deprecated in v1.5 and will\n",
            "                        be removed in 1.7. Please use ``detect_anomaly``\n",
            "                        instead.\n",
            "\n",
            "Data Args:\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Number of examples to operate on per forward step.\n",
            "                        Default is 128.\n",
            "  --num_workers NUM_WORKERS\n",
            "                        Number of additional processes to load data. Default\n",
            "                        is 8.\n",
            "\n",
            "Model Args:\n",
            "  --fc1 FC1\n",
            "  --fc2 FC2\n",
            "  --fc_dropout FC_DROPOUT\n",
            "\n",
            "LitModel Args:\n",
            "  --optimizer OPTIMIZER\n",
            "                        optimizer class from torch.optim\n",
            "  --lr LR\n",
            "  --one_cycle_max_lr ONE_CYCLE_MAX_LR\n",
            "  --one_cycle_total_steps ONE_CYCLE_TOTAL_STEPS\n",
            "  --loss LOSS           loss function from torch.nn.functional\n"
          ]
        }
      ],
      "source": [
        "%run training/run_experiment.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab02b_cnn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
